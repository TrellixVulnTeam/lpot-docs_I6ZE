
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Metrics &#8212; Intel® Low Precision Optimization Tool  documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LPOT UX" href="ux.html" />
    <link rel="prev" title="Dataset" href="dataset.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../index.html">
<p class="title">Intel® Low Precision Optimization Tool</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../README.html">
  Introduction
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="doclist.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../releases_info.html">
  Releases
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../contributions.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../legal_information.html">
  Legal
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../security_policy.html">
  Security
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference external nav-link" href="https://github.com/intel/lpot">
  GitHub
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-use-metrics">
   How to use Metrics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#config-built-in-metric-in-a-yaml-file">
     Config built-in metric in a yaml file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#config-custom-metric-in-code">
     Config custom metric in code
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#built-in-metric-support-list">
   Built-in metric support list
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensorflow">
     TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch">
     PyTorch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mxnet">
     MXNet
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnxrt">
     ONNXRT
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="metrics">
<h1>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">¶</a></h1>
<p>In terms of evaluating the performance of a specific model, we should have general metrics to measure the performance of different models. Different frameworks always have their own Metric module but with different features and APIs. LPOT Metrics supports code-free configuration through a yaml file, with built-in metrics, so that LPOT can achieve performance and accuracy without code changes from the user. In special cases, users can also register their own metric classes through the LPOT method.</p>
<section id="how-to-use-metrics">
<h2>How to use Metrics<a class="headerlink" href="#how-to-use-metrics" title="Permalink to this headline">¶</a></h2>
<section id="config-built-in-metric-in-a-yaml-file">
<h3>Config built-in metric in a yaml file<a class="headerlink" href="#config-built-in-metric-in-a-yaml-file" title="Permalink to this headline">¶</a></h3>
<p>Users can specify an LPOT built-in metric such as shown below:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">evaluation</span><span class="p">:</span>
  <span class="nt">accuracy</span><span class="p">:</span>
    <span class="nt">metric</span><span class="p">:</span>
      <span class="nt">topk</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</section>
<section id="config-custom-metric-in-code">
<h3>Config custom metric in code<a class="headerlink" href="#config-custom-metric-in-code" title="Permalink to this headline">¶</a></h3>
<p>Users can also register their own metric as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Metric</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># init code here</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># add preds and labels to storage</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># clear preds and labels storage</span>

    <span class="k">def</span> <span class="nf">result</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># calculate accuracy</span>
        <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>The result() function returns a higher-is-better scalar to reflect model accuracy on an evaluation dataset.</p>
<p>After defining the metric class, users need to register it with a user-defined metric name and the metric class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">from</span> <span class="nn">lpot.quantization</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Metric</span><span class="p">(</span><span class="n">NewMetric</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">dataloader</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="built-in-metric-support-list">
<h2>Built-in metric support list<a class="headerlink" href="#built-in-metric-support-list" title="Permalink to this headline">¶</a></h2>
<p>LPOT supports some built-in metrics that are popularly used in industry.</p>
<p>Refer to <a class="reference external" href="https://github.com/intel/lpot/tree/master/examples/helloworld/tf_example1">this HelloWorld example</a> on how to config a built-in metric.</p>
<section id="tensorflow">
<h3>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE(compare_label)</td>
<td align="left">compare_label (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE(compare_label)</td>
<td align="left">compare_label (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Square Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td align="left">MSE(compare_label)</td>
<td align="left">compare_label (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
<tr>
<td align="left">mAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path (str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. <br>iou_thrs (float or str, default=0.5): Minimal value for intersection over union that allows to make decision that prediction bounding box is true positive. You can specify one float value between 0 to 1 or string "05:0.05:0.95" for standard COCO thresholds.<br> map_points (int, default=0): The way to calculate mAP. 101 for 101-point interpolated AP, 11 for 11-point interpolated AP, 0 for area under PR curve.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; mAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; iou_thrs: 0.5 <br> &ensp;&ensp;&ensp;&ensp; map_points: 0 <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">COCOmAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path (str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. <br>iou_thrs (float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> map_points (int): The way to calculate mAP. Set to 101 for 101-point interpolated AP.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">VOCmAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. <br>iou_thrs(float or str): Intersection over union threshold. Set to 0.5.<br> map_points(int): The way to calculate mAP. The way to calculate mAP. Set to 0 for area under PR curve.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; VOCmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">BLEU()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">BLEU score computation between labels and predictions. An approximate BLEU scoring method since we do not glue word pieces or decode the ids and tokenize the output. By default, we use ngram order of 4 and use brevity penalty. Also, this does not have beam search</td>
<td align="left">metric: <br> &ensp;&ensp; BLEU: {}</td>
</tr>
<tr>
<td align="left">SquadF1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Evaluate v1.1 of the SQuAD dataset</td>
<td align="left">metric: <br> &ensp;&ensp; SquadF1: {}</td>
</tr>
</tbody>
</table></section>
<section id="pytorch">
<h3>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the top-k categorical accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the accuracy for binary, multiclass and multilabel data. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy">Pytorch docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: <br> &ensp;&ensp;&ensp;&ensp; compare_label： True</td>
</tr>
<tr>
<td align="left">RMSE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squared Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td align="left">MSE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></section>
<section id="mxnet">
<h3>MXNet<a class="headerlink" href="#mxnet" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.Accuracy">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MAE">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squared Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td align="left">MSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MSE">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.F1">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></section>
<section id="onnxrt">
<h3>ONNXRT<a class="headerlink" href="#onnxrt" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: <br> &ensp;&ensp;&ensp;&ensp; compare_label： True</td>
</tr>
<tr>
<td align="left">RMSE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squared Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td align="left">MSE(compare_label)</td>
<td align="left">compare_label(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
<tr>
<td align="left">mAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. <br>iou_thrs(float or str, default=0.5): Minimal value for intersection over union that allows to make decision that prediction bounding box is true positive. You can specify one float value between 0 to 1 or string "05:0.05:0.95" for standard COCO thresholds.<br> map_points(int, default=0): The way to calculate mAP. 101 for 101-point interpolated AP, 11 for 11-point interpolated AP, 0 for area under PR curve.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; mAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; iou_thrs: 0.5 <br> &ensp;&ensp;&ensp;&ensp; map_points: 0 <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">COCOmAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. <br>iou_thrs(float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> map_points(int): The way to calculate mAP. Set to 101 for 101-point interpolated AP.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">VOCmAP( anno_path, iou_thrs, map_points)</td>
<td align="left">anno_path(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/label_map.yaml">label_map</a> for its format .<br>iou_thrs(float or str): Intersection over union threshold. Set to 0.5.<br> map_points(int): The way to calculate mAP. The way to calculate mAP. Set to 0 for area under PR curve.</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; VOCmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td align="left">GLUE(task)</td>
<td align="left">task (str, default=mrpc): The name of the task. Choices include mrpc, qqp, qnli, rte, sts-b, cola, mnli, wnli.</td>
<td align="left">preds, labels</td>
<td align="left">Computes GLUE score for bert model.</td>
<td align="left">metric: <br> &ensp;&ensp; GLUE: <br> &ensp;&ensp;&ensp;&ensp; task: mrpc</td>
</tr>
</tbody>
</table></section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="dataset.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Dataset</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ux.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">LPOT UX</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel® Low Precision Optimization Tool.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>