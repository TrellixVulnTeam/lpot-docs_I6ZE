
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Adaptor &#8212; Intel® Low Precision Optimization Tool  documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tuning Strategies" href="tuning_strategies.html" />
    <link rel="prev" title="TensorBoard" href="tensorboard.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../index.html">
<p class="title">Intel® Low Precision Optimization Tool</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../README.html">
  Introduction
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="doclist.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../releases_info.html">
  Releases
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../contributions.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../legal_information.html">
  Legal
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../security_policy.html">
  Security
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference external nav-link" href="https://github.com/intel/lpot">
  GitHub
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaptor-design">
   Adaptor Design
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#query-api">
     Query API
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#background">
       Background
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unify-config-introduction">
       Unify Config Introduction
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#query-api-introduction">
       Query API Introduction
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#customize-a-new-framework-backend">
   Customize a New Framework Backend
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="adaptor">
<h1>Adaptor<a class="headerlink" href="#adaptor" title="Permalink to this headline">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Intel® Low Precision Optimization Tool (LPOT) built the low-precision inference
solution on popular Deep Learning frameworks such as TensorFlow, PyTorch,
MXNet, and ONNX Runtime. The adaptor layer is the bridge between the LPOT
tuning strategy and vanilla framework quantization APIs.</p>
</section>
<section id="adaptor-design">
<h2>Adaptor Design<a class="headerlink" href="#adaptor-design" title="Permalink to this headline">¶</a></h2>
<p>LPOT supports a new adaptor extension by
implementing a subclass <code class="docutils literal notranslate"><span class="pre">Adaptor</span></code> class in the lpot.adaptor package
and registering this strategy by the <code class="docutils literal notranslate"><span class="pre">adaptor_registry</span></code> decorator.</p>
<p>For example, a user can implement an <code class="docutils literal notranslate"><span class="pre">Abc</span></code> adaptor like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@adaptor_registry</span>
<span class="k">class</span> <span class="nc">AbcAdaptor</span><span class="p">(</span><span class="n">Adaptor</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">framework_specific_info</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tune_cfg</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">q_func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">postprocess</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">metric</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">measurer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">iteration</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tensorboard</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">query_fw_capability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">query_fused_patterns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">quantize</span></code> function is used to perform calibration and quantization in post-training quantization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">evaluate</span></code> function is used to run an evaluation on a validation dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">query_fw_capability</span></code> function is used to run a query framework quantization capability and intersects with the user yaml configuration.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">query_fused_patterns</span></code> function is used to run a query framework graph fusion capability and decide the fusion tuning space.</p></li>
</ul>
<section id="query-api">
<h3>Query API<a class="headerlink" href="#query-api" title="Permalink to this headline">¶</a></h3>
<section id="background">
<h4>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h4>
<p>Besides the adaptor API, we also introduced the Query API which describes the
behavior of a specific framework. With this API, LPOT can easily query the
following information on the current runtime framework.</p>
<ul class="simple">
<li><p>The runtime version information.</p></li>
<li><p>The Quantizable ops type.</p></li>
<li><p>The supported sequence of each quantizable op.</p></li>
<li><p>The instance of each sequence.</p></li>
</ul>
<p>In the past, the above information was generally defined and hidden in every corner of the code which made effective maintenance difficult. With the Query API, we only need to create one unified yaml file and call the corresponding API to get the information. For example, the <a class="reference external" href="https://github.com/intel/lpot/blob/master/docs/../lpot/adaptor/tensorflow.yaml">tensorflow.yaml</a> keeps the current Tensorflow framework ability. We recommend that the end user not make modifications if requirements are not clear.</p>
</section>
<section id="unify-config-introduction">
<h4>Unify Config Introduction<a class="headerlink" href="#unify-config-introduction" title="Permalink to this headline">¶</a></h4>
<p>Below is a fragment of the Tensorflow configuration file.</p>
<ul class="simple">
<li><p><strong>precisions</strong> field defines the supported precision for LPOT.</p>
<ul>
<li><p>valid_mixed_precision enumerates all supported precision combinations for specific scenario. For example, if one hardware doesn’t support bf16， it should be <code class="docutils literal notranslate"><span class="pre">int8</span> <span class="pre">+</span> <span class="pre">fp32</span></code>.</p></li>
</ul>
</li>
<li><p><strong>ops</strong> field defines the valid OP type list for each precision.</p></li>
<li><p><strong>capabilities</strong> field focuses on the quantization ability of specific ops such as granularity, scheme, and algorithm. The activation assumes the same data type for both input and output activation by default based on op semantics defined by frameworks.</p></li>
<li><p><strong>patterns</strong> field defines the supported fusion sequence of each op.</p></li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="p p-Indicator">-</span>
  <span class="nt">version</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="s">&#39;2.4.0&#39;</span>
  
  <span class="nt">precisions</span><span class="p">:</span> <span class="nl">&amp;common_precisions</span>
    <span class="nt">names</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">int8, uint8, bf16, fp32</span>
    <span class="nt">valid_mixed_precisions</span><span class="p">:</span> <span class="p p-Indicator">[]</span>
  
  <span class="nt">ops</span><span class="p">:</span> <span class="nl">&amp;common_ops</span>
    <span class="nt">int8</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;Conv2D&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;MatMul&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;ConcatV2&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;MaxPool&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;AvgPool&#39;</span><span class="p p-Indicator">]</span>
    <span class="nt">uint8</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;Conv2D&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;DepthwiseConv2dNative&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;MatMul&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;ConcatV2&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;MaxPool&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;AvgPool&#39;</span><span class="p p-Indicator">]</span>
    <span class="nt">bf16</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;Conv2D&#39;</span><span class="p p-Indicator">]</span>  <span class="c1">#TODO need to add more bf16 op types here</span>
    <span class="nt">fp32</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;*&#39;</span><span class="p p-Indicator">]</span> <span class="c1"># &#39;*&#39; means all op types</span>
  
  <span class="nt">capabilities</span><span class="p">:</span> <span class="nl">&amp;common_capabilities</span>
    <span class="nt">int8</span><span class="p">:</span> <span class="nl">&amp;ref_2_4_int8</span> <span class="p p-Indicator">{</span>
          <span class="s">&#39;Conv2D&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_channel&#39;</span><span class="p p-Indicator">,</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">},</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;kl&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="s">&#39;MatMul&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;kl&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">},</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;asym&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="s">&#39;default&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="p p-Indicator">}</span>

    <span class="nt">uint8</span><span class="p">:</span> <span class="nl">&amp;ref_2_4_uint8</span> <span class="p p-Indicator">{</span>
          <span class="s">&#39;Conv2D&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_channel&#39;</span><span class="p p-Indicator">,</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">},</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;kl&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="s">&#39;MatMul&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;kl&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">},</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;asym&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="s">&#39;default&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
            <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span>
                        <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;scheme&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;sym&#39;</span><span class="p p-Indicator">],</span>
                        <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_tensor&#39;</span><span class="p p-Indicator">]</span>
                        <span class="p p-Indicator">}</span>
                    <span class="p p-Indicator">},</span>
          <span class="p p-Indicator">}</span>

  <span class="nt">patterns</span><span class="p">:</span> <span class="nl">&amp;common_patterns</span>
    <span class="nt">fp32</span><span class="p">:</span> <span class="p p-Indicator">[</span> <span class="c1">#TODO Add more patterns here to demonstrate our concept the results external engine should return.</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd&#39;</span>
        <span class="p p-Indicator">]</span>
    <span class="nt">int8</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">]</span>
    <span class="nt">uint8</span><span class="p">:</span> <span class="p p-Indicator">[</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">AddN</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">AddN</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">AddV2</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">AddV2</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;Conv2D</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;DepthwiseConv2dNative</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;DepthwiseConv2dNative</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Add</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu6&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;DepthwiseConv2dNative</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;MatMul</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">Relu&#39;</span><span class="p p-Indicator">,</span>
        <span class="s">&#39;MatMul</span><span class="nv"> </span><span class="s">+</span><span class="nv"> </span><span class="s">BiasAdd&#39;</span><span class="p p-Indicator">,</span>
  <span class="p p-Indicator">]</span>
</pre></div>
</div>
</section>
<section id="query-api-introduction">
<h4>Query API Introduction<a class="headerlink" href="#query-api-introduction" title="Permalink to this headline">¶</a></h4>
<p>The abstract class <code class="docutils literal notranslate"><span class="pre">QueryBackendCapability</span></code> is defined in <a class="reference external" href="https://github.com/intel/lpot/blob/master/docs/../lpot/adaptor/query.py">query.py</a>. Each framework should inherit it and implement the member function if needed. Refer to Tensorflow implementation <a class="reference external" href="https://github.com/intel/lpot/blob/master/docs/../lpot/adaptor/tensorflow.py">TensorflowQuery</a>.</p>
</section>
</section>
</section>
<section id="customize-a-new-framework-backend">
<h2>Customize a New Framework Backend<a class="headerlink" href="#customize-a-new-framework-backend" title="Permalink to this headline">¶</a></h2>
<p>Look at onnxruntime as an example. ONNX Runtime is a backend proposed by Microsoft, and is based on the MLAS kernel by default.
Onnxruntime already has <a class="reference external" href="https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization">quantization tools</a>, so the question becomes how to integrate onnxruntime quantization tools into LPOT.</p>
<ol>
<li><p>Capability</p>
<p>The user should explore quantization capability first. According to <a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/503b61d897074a494f5798069308ee67d8fb9ace/onnxruntime/python/tools/quantization/onnx_quantizer.py#L77">onnx_quantizer</a>, the quantization tools support the following attributes:</p>
<ul class="simple">
<li><p>whether per_channel</p></li>
<li><p>whether reduce_range</p></li>
<li><p>QLinear mode or Integer mode (which is only seen in onnxruntime)</p></li>
<li><p>whether static (static quantization or dynamic quantization)</p></li>
<li><p>weight_qtype (choices are float32, int8 and uint8)</p></li>
<li><p>input_qtype (choices are float32, int8 and uint8)</p></li>
<li><p>quantization_params (None if dynamic quantization)</p></li>
<li><p>&amp;1.8 nodes_to_quantize, nodes_to_exclude</p></li>
<li><p>op_types_to_quantize</p></li>
</ul>
<p>We can pass a tune capability to LPOT such as:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">{</span><span class="s">&#39;optypewise&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="s">&#39;conv&#39;</span><span class="p p-Indicator">:</span> 
                <span class="p p-Indicator">{</span>
                 <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]},</span>
                 <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]},</span>
                 <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="p p-Indicator">],</span>
                 <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_channel&#39;</span><span class="p p-Indicator">]</span>
                <span class="p p-Indicator">},</span> 
                <span class="s">&#39;matmul&#39;</span><span class="p p-Indicator">:</span> 
                <span class="p p-Indicator">{</span>
                 <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]},</span>
                 <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]},</span>
                 <span class="s">&#39;algorithm&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;minmax&#39;</span><span class="p p-Indicator">,</span> <span class="p p-Indicator">],</span>
                 <span class="s">&#39;granularity&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;per_channel&#39;</span><span class="p p-Indicator">]</span>
                <span class="p p-Indicator">}</span>
                <span class="p p-Indicator">},</span> 
 <span class="s">&#39;opwise&#39;</span><span class="p p-Indicator">:</span>  <span class="p p-Indicator">{</span><span class="nv">(&#39;conv1&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;conv&#39;</span><span class="nt">)</span><span class="p">:</span>
                <span class="p p-Indicator">{</span>
                 <span class="s">&#39;activation&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;uint8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]},</span>
                 <span class="s">&#39;weight&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="s">&#39;dtype&#39;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="s">&#39;int8&#39;</span><span class="p p-Indicator">,</span> <span class="s">&#39;fp32&#39;</span><span class="p p-Indicator">]}</span>
                <span class="p p-Indicator">}</span>
                <span class="p p-Indicator">}</span>
 <span class="p p-Indicator">}</span>
</pre></div>
</div>
</li>
<li><p>Parse tune config</p>
<p>LPOT can generate a tune config from your tune capability such as the
following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span> {
     &#39;fuse&#39;: {&#39;int8&#39;: [[&#39;CONV2D&#39;, &#39;RELU&#39;, &#39;BN&#39;], [&#39;CONV2D&#39;, &#39;RELU&#39;]],
     &#39;fp32&#39;: [[&#39;CONV2D&#39;, &#39;RELU&#39;, &#39;BN&#39;]]}, 
     &#39;calib_iteration&#39;: 10,
     &#39;op&#39;: {
     [&#39;op1&#39;, &#39;CONV2D&#39;]: {
         &#39;activation&#39;:  {&#39;dtype&#39;: &#39;uint8&#39;,
                         &#39;algorithm&#39;: &#39;minmax&#39;,
                         &#39;scheme&#39;:&#39;sym&#39;,
                         &#39;granularity&#39;: &#39;per_tensor&#39;},
         &#39;weight&#39;: {&#39;dtype&#39;: &#39;int8&#39;,
                     &#39;algorithm&#39;: &#39;kl&#39;,
                     &#39;scheme&#39;:&#39;asym&#39;,
                     &#39;granularity&#39;: &#39;per_channel&#39;}
     },
     [&#39;op2&#39;, &#39;RELU]: {
         &#39;activation&#39;: {&#39;dtype&#39;: &#39;int8&#39;,
                         &#39;scheme&#39;: &#39;asym&#39;,
                         &#39;granularity&#39;: &#39;per_tensor&#39;,
                         &#39;algorithm&#39;: &#39;minmax&#39;}
     },
     [&#39;op3&#39;, &#39;CONV2D&#39;]: {
         &#39;activation&#39;:  {&#39;dtype&#39;: &#39;fp32&#39;},
         &#39;weight&#39;: {&#39;dtype&#39;: &#39;fp32&#39;}
     },
     ...
     }
 }
</pre></div>
</div>
<p>Then you can parse this config into a format that ONNXQuantizer can accept.
Verify whether your quantization API supports model wise or op wise quantization. For example, node “conv1” uses the “minmax” algorithm and node “conv2” uses the “KL” algorithm, or the whole model must use “minmax” or “KL” in general.</p>
</li>
<li><p>Pre-optimize
If your backend supports FP32 graph optimization, you can apply it in <strong>query_fw_capability</strong> and quantize your optimized fp32 model instead of
the original model:</p>
<blockquote>
<div><p>model = self.pre_optimized_model if self.pre_optimized_model else model</p>
</div></blockquote>
</li>
<li><p>Do quantization</p>
<p>This part depends on your backend implementations. Refer to <a class="reference external" href="https://github.com/intel/lpot/blob/master/docs/../lpot/adaptor/onnxrt.py">onnxruntime</a> as an example.</p>
</li>
</ol>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="tensorboard.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">TensorBoard</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="tuning_strategies.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tuning Strategies</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel® Low Precision Optimization Tool.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>